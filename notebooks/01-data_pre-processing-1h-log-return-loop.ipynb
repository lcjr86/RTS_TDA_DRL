{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary libraries. Pandas and NumPy for data manipulation, and statsmodels for statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.modwt import *\n",
    "\n",
    "import pywt\n",
    "from pywt import wavedec\n",
    "from pywt import Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_data_range = \"1H\"\n",
    "from_date = '2021-12-31 00:00:00' # 2020-12-31 00:00:00\n",
    "until_date = '2025-07-31 00:00:00'\n",
    "until_date_2 = '2025-08-01 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_data_name = [\"BTCUSDT_1h\", \"ETHUSDT_1h\", \"BNBUSDT_1h\", \"XRPUSDT_1h\", \"ADAUSDT_1h\", \"LTCUSDT_1h\"]\n",
    "list_data_name = [\"ADAUSDT_1h\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/_j22qz4n5j72grfwwy9xgbxc0000gn/T/ipykernel_97728/3778662857.py:19: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  my_range = pd.date_range(start=data.index.min(), end=data.index.max(), freq=ref_data_range)\n",
      "/var/folders/s0/_j22qz4n5j72grfwwy9xgbxc0000gn/T/ipykernel_97728/3778662857.py:37: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  my_range_new = pd.date_range(start=data.index.min(), end=data.index.max(), freq=ref_data_range)\n"
     ]
    }
   ],
   "source": [
    "for data_name in list_data_name:\n",
    "\n",
    "    # Import data\n",
    "    ccy = data_name.split('_')[0]\n",
    "\n",
    "    # data = pd.read_csv(f'../data/{data_name}_data_from_20180101_to_20230102.csv', parse_dates=['date'], index_col='date')\n",
    "    data = pd.read_csv(f'../data/{data_name}_data_from_20210101_to_20251031.csv', parse_dates=['date'], index_col='date')\n",
    "\n",
    "    # Get data from_date until until_date inclusive\n",
    "    data = data[(data.index >= from_date) & (data.index <= until_date_2)]\n",
    "\n",
    "    # Add column with original close\n",
    "    data['original_close'] = data['close']\n",
    "\n",
    "    # Handling missing data \n",
    "\n",
    "    # Check that there is no missing data\n",
    "    # data.index = pd.to_datetime(data.index)\n",
    "    my_range = pd.date_range(start=data.index.min(), end=data.index.max(), freq=ref_data_range)\n",
    "\n",
    "    # Get all the dates in the start_date and end_date range\n",
    "    df_full_dates = pd.DataFrame()\n",
    "    df_full_dates['date'] = my_range\n",
    "\n",
    "    # Remove the data['date'] values that are not in df_full_dates['date']\n",
    "    data = data[data.index.isin(df_full_dates['date'])]\n",
    "\n",
    "    data_new = data.join(df_full_dates.set_index('date'), on='date', how='outer')\n",
    "    data_new.sort_values(by=['date'], inplace=True)\n",
    "\n",
    "    # data_new['date'] = pd.to_datetime(data_new['date'])  # Convert 'date' column to datetime type if needed\n",
    "    data = data_new.copy()  # Make a copy of the dataframe\n",
    "    # data = data.set_index('date')  # Set 'date' as the index for convenient time-based operations\n",
    "    # data_filled = data_new.fillna(method='ffill')\n",
    "    # data_filled = data_new.reset_index()    \n",
    "\n",
    "    my_range_new = pd.date_range(start=data.index.min(), end=data.index.max(), freq=ref_data_range)\n",
    "\n",
    "    data_interp_temp = data.copy()\n",
    "    data_interp_temp.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    data_interp_temp['date_ordinal'] = data_interp_temp['date'].apply(lambda x: x.timestamp())\n",
    "\n",
    "    known_data = data_interp_temp.dropna()\n",
    "\n",
    "    cs = CubicSpline(known_data['date_ordinal'], known_data['close'])\n",
    "\n",
    "    missing_data_indices = data_interp_temp[data_interp_temp['close'].isnull()].index\n",
    "\n",
    "    interpolated_values = cs(data_interp_temp.loc[missing_data_indices, 'date_ordinal'])\n",
    "\n",
    "    data_interp_temp.loc[missing_data_indices, 'close'] = interpolated_values\n",
    "\n",
    "    data_interp_temp.set_index('date', inplace=True)\n",
    "\n",
    "    data = data_interp_temp.copy()\n",
    "    # replace the original data with the interpolated data by using the index\n",
    "    data['processed_close'] = data_interp_temp['close']\n",
    "\n",
    "    # add log return\n",
    "    data['processed_log_return'] = np.log(data['close'] / data['close'].shift(1))\n",
    "\n",
    "    # Remove the NaN values\n",
    "    data = data[~data['processed_log_return'].isnull()]\n",
    "\n",
    "    # Outliers\n",
    "\n",
    "    # Compute Z-score for the 'close' price column\n",
    "    z_scores = np.abs((data['processed_log_return'] - data['processed_log_return'].mean()) / data['processed_log_return'].std())\n",
    "\n",
    "    # Assuming df is your DataFrame and 'log_return' is the column with outliers\n",
    "    upper_bound = data['processed_log_return'].quantile(0.98)\n",
    "    lower_bound = data['processed_log_return'].quantile(0.02)\n",
    "\n",
    "    # Applying capping and flooring\n",
    "    data['outliers_processed_log_return'] = data['processed_log_return'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "    # z_scores = np.abs((data['outliers_processed_log_return'] - data['outliers_processed_log_return'].mean()) / data['outliers_processed_log_return'].std())\n",
    "\n",
    "    # Normalization\n",
    "\n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    data[['normalized_outliers_processed_log_return']] = scaler.fit_transform(data[['outliers_processed_log_return']])\n",
    "\n",
    "    # Denoising\n",
    "\n",
    "    wt = modwt(data['normalized_outliers_processed_log_return'], 'db2', 5)\n",
    "    wtmra = modwtmra(wt, 'db2')\n",
    "\n",
    "    df_wtmra = pd.DataFrame(wtmra)\n",
    "    df_wtmra = df_wtmra.transpose()\n",
    "\n",
    "    # Rename the df_wtrma columns\n",
    "    df_wtmra.columns = ['processed_log_return_wtmra_0', 'processed_log_return_wtmra_1', 'processed_log_return_wtmra_2', 'processed_log_return_wtmra_3', 'processed_log_return_wtmra_4', 'processed_log_return_wtmra_5']\n",
    "\n",
    "    df_wtmra['processed_log_return_wtmra_5_4'] = df_wtmra['processed_log_return_wtmra_5'] + df_wtmra['processed_log_return_wtmra_4']\n",
    "    df_wtmra['processed_log_return_wtmra_5_4_3'] = df_wtmra['processed_log_return_wtmra_5_4'] + df_wtmra['processed_log_return_wtmra_3']\n",
    "    df_wtmra['processed_log_return_wtmra_5_4_3_2'] = df_wtmra['processed_log_return_wtmra_5_4_3'] + df_wtmra['processed_log_return_wtmra_2']\n",
    "    df_wtmra['processed_log_return_wtmra_5_4_3_2_1'] = df_wtmra['processed_log_return_wtmra_5_4_3_2'] + df_wtmra['processed_log_return_wtmra_1']\n",
    "    df_wtmra['processed_log_return_wtmra_5_4_3_2_1_0'] = df_wtmra['processed_log_return_wtmra_5_4_3_2_1'] + df_wtmra['processed_log_return_wtmra_0']\n",
    "\n",
    "    df_wtmra['processed_log_return_wtmra_0_1'] = df_wtmra['processed_log_return_wtmra_0'] + df_wtmra['processed_log_return_wtmra_1']\n",
    "    df_wtmra['processed_log_return_wtmra_0_1_2'] = df_wtmra['processed_log_return_wtmra_0_1'] + df_wtmra['processed_log_return_wtmra_2']\n",
    "    df_wtmra['processed_log_return_wtmra_0_1_2_3'] = df_wtmra['processed_log_return_wtmra_0_1_2'] + df_wtmra['processed_log_return_wtmra_3']\n",
    "    df_wtmra['processed_log_return_wtmra_0_1_2_3_4'] = df_wtmra['processed_log_return_wtmra_0_1_2_3'] + df_wtmra['processed_log_return_wtmra_4']\n",
    "    df_wtmra['processed_log_return_wtmra_0_1_2_3_4_5'] = df_wtmra['processed_log_return_wtmra_0_1_2_3_4'] + df_wtmra['processed_log_return_wtmra_5']\n",
    "\n",
    "    # Add the data['date'] column to the df_wtmra DataFrame\n",
    "    df_wtmra['date'] = data.index\n",
    "    # Set the index of df_wtmra to be the date column\n",
    "    df_wtmra.set_index('date', inplace=True)\n",
    "\n",
    "    # Add df_wtmra colums to the data DataFrame by joining on the date index\n",
    "    data = data.join(df_wtmra)\n",
    "\n",
    "    # Remove NANs\n",
    "    data.processed_close.dropna(inplace=True)\n",
    "\n",
    "    # Get data from_date until until_date inclusive\n",
    "    data = data[(data.index >= from_date) & (data.index <= until_date)]\n",
    "\n",
    "\n",
    "    # Save the processed data\n",
    "    data.to_csv(f'../data/01-output-{data_name}-from-{from_date}-until-{until_date}-log-return.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
