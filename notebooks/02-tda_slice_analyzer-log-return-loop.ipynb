{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.data_utils import *\n",
    "from src.tda_utils import *\n",
    "from src.utils import *\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_data_name = [\"BTCUSDT_1d\", \"ETHUSDT_1d\", \"BNBUSDT_1d\", \"XRPUSDT_1d\", \"ADAUSDT_1d\", \"LTCUSDT_1d\"]                   \n",
    "# list_data_name = [\"BTCUSDT_1h\", \"ETHUSDT_1h\", \"BNBUSDT_1h\", \"XRPUSDT_1h\", \"ADAUSDT_1h\", \"LTCUSDT_1h\"]                   \n",
    "\n",
    "list_data_name = [\"ADAUSDT_1d\"]                   \n",
    "                        \n",
    "from_date = '2021-12-31 00:00:00' # 2020-12-31 00:00:00\n",
    "until_date = '2025-07-31 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_slice_position = 0\n",
    "list_slice_size_in_days = [14, 21, 30, 45]\n",
    "# list_slice_size_in_days = [45]\n",
    "# list_slice_size_in_days = [14, 21, 30]\n",
    "num_point_for_1d = (1 * 24) # 1h\n",
    "column_name = 'processed_log_return_wtmra_0'\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the point cloud creation\n",
    "dimension = 2\n",
    "list_time_delay = [3, 7]\n",
    "# time_delay_in_days = 7\n",
    "stride = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for saving results if doesn't exist\n",
    "def create_folder(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def erase_files(path): \n",
    "    files = glob.glob(path + '*')\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/_j22qz4n5j72grfwwy9xgbxc0000gn/T/ipykernel_13566/2204231434.py:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_features = pd.concat([df_features, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to ../results/processed_log_return_wtmra_0_ADAUSDT_1d_time_delay_72_slice_size_336/features//features_ADAUSDT_1d_time_delay_72_slice_size_336.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/_j22qz4n5j72grfwwy9xgbxc0000gn/T/ipykernel_13566/2204231434.py:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_features = pd.concat([df_features, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to ../results/processed_log_return_wtmra_0_ADAUSDT_1d_time_delay_168_slice_size_336/features//features_ADAUSDT_1d_time_delay_168_slice_size_336.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/_j22qz4n5j72grfwwy9xgbxc0000gn/T/ipykernel_13566/2204231434.py:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_features = pd.concat([df_features, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to ../results/processed_log_return_wtmra_0_ADAUSDT_1d_time_delay_72_slice_size_504/features//features_ADAUSDT_1d_time_delay_72_slice_size_504.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/_j22qz4n5j72grfwwy9xgbxc0000gn/T/ipykernel_13566/2204231434.py:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_features = pd.concat([df_features, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to ../results/processed_log_return_wtmra_0_ADAUSDT_1d_time_delay_168_slice_size_504/features//features_ADAUSDT_1d_time_delay_168_slice_size_504.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/_j22qz4n5j72grfwwy9xgbxc0000gn/T/ipykernel_13566/2204231434.py:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_features = pd.concat([df_features, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to ../results/processed_log_return_wtmra_0_ADAUSDT_1d_time_delay_72_slice_size_720/features//features_ADAUSDT_1d_time_delay_72_slice_size_720.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/_j22qz4n5j72grfwwy9xgbxc0000gn/T/ipykernel_13566/2204231434.py:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_features = pd.concat([df_features, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to ../results/processed_log_return_wtmra_0_ADAUSDT_1d_time_delay_168_slice_size_720/features//features_ADAUSDT_1d_time_delay_168_slice_size_720.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/_j22qz4n5j72grfwwy9xgbxc0000gn/T/ipykernel_13566/2204231434.py:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_features = pd.concat([df_features, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to ../results/processed_log_return_wtmra_0_ADAUSDT_1d_time_delay_72_slice_size_1080/features//features_ADAUSDT_1d_time_delay_72_slice_size_1080.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/_j22qz4n5j72grfwwy9xgbxc0000gn/T/ipykernel_13566/2204231434.py:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_features = pd.concat([df_features, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to ../results/processed_log_return_wtmra_0_ADAUSDT_1d_time_delay_168_slice_size_1080/features//features_ADAUSDT_1d_time_delay_168_slice_size_1080.csv\n"
     ]
    }
   ],
   "source": [
    "for data_name in list_data_name:\n",
    "    for slice_size_in_days in list_slice_size_in_days:\n",
    "        slice_size = slice_size_in_days * num_point_for_1d\n",
    "        for time_delay in list_time_delay:\n",
    "            time_delay_in_days = (time_delay * 24)\n",
    "\n",
    "            # Load the data\n",
    "            data = pd.read_csv(f'../data/01-output-{data_name}-from-{from_date}-until-{until_date}-log-return.csv', parse_dates=['date'], index_col='date')\n",
    "\n",
    "            df = data[column_name]\n",
    "\n",
    "            full_series_with_highlight_path = f'../results/{column_name}_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}/full_series_with_highlight/'\n",
    "            sliced_time_series_path = f'../results/{column_name}_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}/sliced_time_series/'\n",
    "            point_cloud_path = f'../results/{column_name}_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}/point_cloud/'\n",
    "            persistence_diagram_path = f'../results/{column_name}_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}/persistence_diagram/'\n",
    "            mapper_graph_path = f'../results/{column_name}_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}/mapper_graph/'\n",
    "            mosaic_path = f'../results/{column_name}_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}/mosaic/'\n",
    "\n",
    "            features_path = f'../results/{column_name}_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}/features/'\n",
    "            features_plot_path = f'../results/{column_name}_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}/features_plot/'\n",
    "            mosaic_and_features_path = f'../results/{column_name}_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}/mosaic_and_features/'\n",
    "            video_path = f'../results/{column_name}_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}/video/'\n",
    "\n",
    "\n",
    "            if save:\n",
    "                create_folder(full_series_with_highlight_path)\n",
    "                create_folder(sliced_time_series_path)\n",
    "                create_folder(point_cloud_path)\n",
    "                create_folder(persistence_diagram_path)\n",
    "                create_folder(mapper_graph_path)\n",
    "                create_folder(mosaic_path)\n",
    "                create_folder(features_path)\n",
    "                create_folder(features_plot_path)\n",
    "                create_folder(mosaic_and_features_path)\n",
    "                create_folder(video_path)\n",
    "\n",
    "            if save:\n",
    "                erase_files(full_series_with_highlight_path)\n",
    "                erase_files(sliced_time_series_path)\n",
    "                erase_files(point_cloud_path)\n",
    "                erase_files(persistence_diagram_path)\n",
    "                erase_files(mapper_graph_path)\n",
    "                erase_files(mosaic_path)\n",
    "                erase_files(features_path)\n",
    "                erase_files(features_plot_path)\n",
    "                erase_files(mosaic_and_features_path)\n",
    "                erase_files(video_path)\n",
    "\n",
    "            df_features = pd.DataFrame()\n",
    "\n",
    "            # Create an empty dataframe with these columns: [initial_slice_position, slice_size, start_date, end_date, connected_components_entropy, loops_entropy, voids_entropy, connected_components_amplitude, loops_amplitude, voids_amplitude, connected_components_number_of_points, loops_number_of_points, voids_number_of_points]\n",
    "            df_features = pd.DataFrame(columns=['initial_slice_position', 'slice_size', 'start_date', 'end_date', 'connected_components_entropy', 'loops_entropy', 'voids_entropy', 'connected_components_amplitude', 'loops_amplitude', 'voids_amplitude', 'connected_components_number_of_points', 'loops_number_of_points', 'voids_number_of_points'])\n",
    "\n",
    "            for i in range(0, (len(data)-slice_size), num_point_for_1d):\n",
    "\n",
    "                # fig = generate_plot_full_series_with_highlight(df, initial_slice_position + i, slice_size, save, full_series_with_highlight_path, f'full_series_with_highlight_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}', data_name)\n",
    "                time_series = get_sliced_time_series(df, initial_slice_position + i, slice_size)\n",
    "                # fig = generate_plot_sliced_time_series(df, initial_slice_position + i, slice_size, save, sliced_time_series_path, f'sliced_time_series_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}')\n",
    "                # fig.show()\n",
    "                point_cloud = create_point_cloud(time_series, dimension, time_delay_in_days, stride)\n",
    "                # fig = generate_plot_point_cloud(point_cloud, initial_slice_position + i, slice_size, save, point_cloud_path, f'point_cloud_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}')\n",
    "                # fig.show()\n",
    "                # fig = generate_plot_persistence_diagram(point_cloud, initial_slice_position + i, slice_size, save, persistence_diagram_path, f'persistence_diagram_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}')\n",
    "                # fig.show()\n",
    "                # fig = plot_persistence_barcode(point_cloud, initial_slice_position + i, slice_size, save, '../results/persistence_barcode/', f'persistence_barcode_{initial_slice_position + i}_{slice_size}')\n",
    "                # fig.show()\n",
    "                # fig = plot_persistence_heatmap(point_cloud, initial_slice_position + i, slice_size, save, '../results/persistence_heatmap/', f'persistence_heatmap_{initial_slice_position + i}_{slice_size}')\n",
    "                # fig.show()\n",
    "                # fig = plot_persistence_landscape(point_cloud, initial_slice_position + i, slice_size, save, '../results/persistence_landscape/', f'persistence_landscape_{initial_slice_position + i}_{slice_size}')\n",
    "                # fig.show()\n",
    "                # fig = generate_plot_mapper_graph(point_cloud, initial_slice_position + i, slice_size, save, mapper_graph_path, f'mapper_graph_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}')\n",
    "                # fig.show()\n",
    "\n",
    "                entropy_features_dict, amplitude_features_dict, number_of_points_features_dict = get_features(point_cloud)\n",
    "\n",
    "                # Add the features to the dataframe using concat\n",
    "                df_features = pd.concat([df_features, pd.DataFrame({\n",
    "                    'initial_slice_position': initial_slice_position + i,\n",
    "                    'slice_size': slice_size,\n",
    "                    'start_date': data.index[initial_slice_position + i],\n",
    "                    'end_date': data.index[initial_slice_position + i + slice_size],\n",
    "                    'connected_components_entropy': entropy_features_dict['connected_components_entropy'],\n",
    "                    'loops_entropy': entropy_features_dict['loops_entropy'],\n",
    "                    'voids_entropy': entropy_features_dict['voids_entropy'],\n",
    "                    'connected_components_amplitude': amplitude_features_dict['connected_components_amplitude'],\n",
    "                    'loops_amplitude': amplitude_features_dict['loops_amplitude'],\n",
    "                    'voids_amplitude': amplitude_features_dict['voids_amplitude'],\n",
    "                    'connected_components_number_of_points': number_of_points_features_dict['connected_components_number_of_points'],\n",
    "                    'loops_number_of_points': number_of_points_features_dict['loops_number_of_points'],\n",
    "                    'voids_number_of_points': number_of_points_features_dict['voids_number_of_points']\n",
    "                }, index=[0])], ignore_index=True)        \n",
    "\n",
    "                # full_path_img1 = f'{sliced_time_series_path}/sliced_time_series_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}.png'\n",
    "                # full_path_img2 = f'{full_series_with_highlight_path}/full_series_with_highlight_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}.png'\n",
    "                # full_path_img3 = f'{point_cloud_path}/point_cloud_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}.png'\n",
    "                # full_path_img4 = f'{point_cloud_path}/point_cloud_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}_fixed_axis.png'\n",
    "                # full_path_img5 = f'{persistence_diagram_path}/persistence_diagram_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}.png'\n",
    "                # full_path_img6 = f'{persistence_diagram_path}/persistence_diagram_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}_fixed_axis.png'\n",
    "                # full_path_img7 = f'{mapper_graph_path}/mapper_graph_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}.png'\n",
    "                # full_path_mosaic = f'{mosaic_path}/mosaic_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}.png'\n",
    "                \n",
    "                # generate_mosaic(full_path_img1, full_path_img2, full_path_img3, full_path_img4, full_path_img5, full_path_img6, full_path_img7, full_path_mosaic)                \n",
    "\n",
    "            # Save the dataframe with the features\n",
    "            if save:\n",
    "                df_features.to_csv(f'{features_path}/features_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}.csv', index=False)\n",
    "                print(f'Features saved to {features_path}/features_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}.csv')\n",
    "\n",
    "            # df_features = pd.read_csv(f'{features_path}/features_{data_name}_time_delay_{time_delay_in_days}_slice_size_{slice_size}.csv')\n",
    "\n",
    "            # list_columns_to_normalize = ['connected_components_entropy', 'loops_entropy', 'voids_entropy',\n",
    "            #     'connected_components_amplitude', 'loops_amplitude', 'voids_amplitude',\n",
    "            #     'connected_components_number_of_points', 'loops_number_of_points',\n",
    "            #     'voids_number_of_points'\n",
    "            #     ]\n",
    "            # df_features_normalized = normalize_dataframe(df_features, list_columns_to_normalize)\n",
    "            # df_features_normalized_v2 = normalize_dataframe(df_features, list_columns_to_normalize, (-1, 1))\n",
    "\n",
    "            # for i in range(0, (len(data)-slice_size), num_point_for_1d):\n",
    "            #     mosaic_path_img = f'{mosaic_path}/mosaic_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}.png'\n",
    "            #     features_path_img = f'{features_plot_path}/features_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}.png'\n",
    "            #     features_accumulate_path_img = f'{features_plot_path}/features_accumulate_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}.png'\n",
    "            #     mosaic_and_features_path_img = f'{mosaic_and_features_path}/mosaic_and_features_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}.png'\n",
    "            #     fig = generate_features_plot(data, column_name, df_features_normalized_v2, initial_slice_position + i, slice_size, save, features_plot_path, f'features_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}')\n",
    "            #     # fig = generate_features_plot(data, column_name, df_features_normalized_accumulated, initial_slice_position + i, slice_size, save, features_plot_path, f'features_accumulate_{generate_numerical_filename(initial_slice_position + i, len(data))}_{slice_size}')\n",
    "            #     generate_mosaic_and_features(mosaic_path_img, features_path_img, mosaic_and_features_path_img)\n",
    "\n",
    "            # create_mosaic_video(f'{mosaic_and_features_path}/', f'{video_path}/', f'mosaic_and_features_video_{slice_size}.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
